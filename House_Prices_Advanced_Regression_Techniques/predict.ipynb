{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train = pd.read_csv(\"data/train.csv\")\n",
    "test = pd.read_csv(\"data/test.csv\")\n",
    "all_X = pd.concat((train.loc[:, 'MSSubClass':'SaleCondition'],\n",
    "                   test.loc[:, 'MSSubClass':'SaleCondition']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numeric_feats = all_X.dtypes[all_X.dtypes != \"object\"].index\n",
    "all_X[numeric_feats] = all_X[numeric_feats].apply(lambda x: (x - x.mean())\n",
    "                                                            / (x.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_X = pd.get_dummies(all_X, dummy_na=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_X = all_X.fillna(all_X.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_train = train.shape[0]\n",
    "\n",
    "X_train = all_X[:num_train].as_matrix()\n",
    "X_test = all_X[num_train:].as_matrix()\n",
    "y_train = train.SalePrice.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mxnet import ndarray as nd\n",
    "from mxnet import autograd\n",
    "from mxnet import gluon\n",
    "\n",
    "X_train = nd.array(X_train)\n",
    "y_train = nd.array(y_train)\n",
    "y_train.reshape((num_train, 1))\n",
    "\n",
    "X_test = nd.array(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "square_loss = gluon.loss.L2Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_rmse_log(net, X_train, y_train):\n",
    "    num_train = X_train.shape[0]\n",
    "    clipped_preds = nd.clip(net(X_train), 1, float('inf'))\n",
    "    return np.sqrt(2 * nd.sum(square_loss(\n",
    "        nd.log(clipped_preds), nd.log(y_train))).asscalar() / num_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_net():\n",
    "    net = gluon.nn.Sequential()\n",
    "    with net.name_scope():\n",
    "        net.add(gluon.nn.Flatten())\n",
    "        net.add(gluon.nn.Dense(200, activation=\"relu\"))\n",
    "        net.add(gluon.nn.Dense(1))\n",
    "    net.initialize()\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.dpi']= 120\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train(net, X_train, y_train, X_test, y_test, epochs,\n",
    "          verbose_epoch, learning_rate, weight_decay):\n",
    "    train_loss = []\n",
    "    if X_test is not None:\n",
    "        test_loss = []\n",
    "    batch_size = 100\n",
    "    \n",
    "    dataset_train = gluon.data.ArrayDataset(X_train, y_train)\n",
    "    data_iter_train = gluon.data.DataLoader(\n",
    "        dataset_train, batch_size,shuffle=True)\n",
    "    trainer = gluon.Trainer(net.collect_params(), 'adam',\n",
    "                            {'learning_rate': learning_rate,\n",
    "                             'wd': weight_decay})\n",
    "    net.collect_params().initialize(force_reinit=True)\n",
    "    for epoch in range(epochs):\n",
    "        for data, label in data_iter_train:\n",
    "            with autograd.record():\n",
    "                output = net(data)\n",
    "                loss = square_loss(output, label)\n",
    "            loss.backward()\n",
    "            trainer.step(batch_size)\n",
    "\n",
    "            cur_train_loss = get_rmse_log(net, X_train, y_train)\n",
    "#         if epoch > verbose_epoch:\n",
    "#             print(\"Epoch %d, train loss: %f\" % (epoch, cur_train_loss))\n",
    "        train_loss.append(cur_train_loss)\n",
    "        if X_test is not None:\n",
    "            cur_test_loss = get_rmse_log(net, X_test, y_test)\n",
    "            test_loss.append(cur_test_loss)\n",
    "#     plt.plot(train_loss)\n",
    "#     plt.legend(['train'])\n",
    "#     if X_test is not None:\n",
    "#         plt.plot(test_loss)\n",
    "#         plt.legend(['train','test'])\n",
    "#     plt.show()\n",
    "    if X_test is not None:\n",
    "        return cur_train_loss, cur_test_loss\n",
    "    else:\n",
    "        return cur_train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def k_fold_cross_valid(k, epochs, verbose_epoch, X_train, y_train,\n",
    "                       learning_rate, weight_decay):\n",
    "    assert k > 1\n",
    "    fold_size = X_train.shape[0] // k\n",
    "    train_loss_sum = 0.0\n",
    "    test_loss_sum = 0.0\n",
    "    for test_i in range(k):\n",
    "        X_val_test = X_train[test_i * fold_size: (test_i + 1) * fold_size, :]\n",
    "        y_val_test = y_train[test_i * fold_size: (test_i + 1) * fold_size]\n",
    "\n",
    "        val_train_defined = False\n",
    "        for i in range(k):\n",
    "            if i != test_i:\n",
    "                X_cur_fold = X_train[i * fold_size: (i + 1) * fold_size, :]\n",
    "                y_cur_fold = y_train[i * fold_size: (i + 1) * fold_size]\n",
    "                if not val_train_defined:\n",
    "                    X_val_train = X_cur_fold\n",
    "                    y_val_train = y_cur_fold\n",
    "                    val_train_defined = True\n",
    "                else:\n",
    "                    X_val_train = nd.concat(X_val_train, X_cur_fold, dim=0)\n",
    "                    y_val_train = nd.concat(y_val_train, y_cur_fold, dim=0)\n",
    "        net = get_net()\n",
    "        train_loss, test_loss = train(\n",
    "            net, X_val_train, y_val_train, X_val_test, y_val_test,\n",
    "            epochs, verbose_epoch, learning_rate, weight_decay)\n",
    "        train_loss_sum += train_loss\n",
    "#         print(\"Test loss: %f\" % test_loss)\n",
    "        test_loss_sum += test_loss\n",
    "    return train_loss_sum / k, test_loss_sum / k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "k = 5\n",
    "epochs = 40\n",
    "verbose_epoch = 95\n",
    "learning_rate = 0.1\n",
    "weight_decay = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-fold validation: Avg train loss: 0.100508, Avg test loss: 0.132985\n"
     ]
    }
   ],
   "source": [
    "train_loss, test_loss = k_fold_cross_valid(k, epochs, verbose_epoch, X_train,\n",
    "                                           y_train, learning_rate, weight_decay)\n",
    "print(\"%d-fold validation: Avg train loss: %f, Avg test loss: %f\" %\n",
    "      (k, train_loss, test_loss))\n",
    "# for epochs in [20,40,60,80]:\n",
    "#     for lr in [0.001,0.01,0.1]:\n",
    "#         for wd in [0,0.01, 0.1, 1, 10, 100,200]:\n",
    "#             train_loss, test_loss = k_fold_cross_valid(k, epochs, verbose_epoch, X_train,\n",
    "#                                                        y_train, lr, wd)\n",
    "#             print(\"epochs,lr, wd\", epochs, lr, wd)\n",
    "#             print(\"%d-fold validation: Avg train loss: %f, Avg test loss: %f\" %\n",
    "#                   (k, train_loss, test_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def learn(epochs, verbose_epoch, X_train, y_train, test, learning_rate,\n",
    "          weight_decay):\n",
    "    net = get_net()\n",
    "    train(net, X_train, y_train, None, None, epochs, verbose_epoch,\n",
    "          learning_rate, weight_decay)\n",
    "    preds = net(X_test).asnumpy()\n",
    "    test['SalePrice'] = pd.Series(preds.reshape(1, -1)[0])\n",
    "    submission = pd.concat([test['Id'], test['SalePrice']], axis=1)\n",
    "    submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn(epochs, verbose_epoch, X_train, y_train, test, learning_rate,\n",
    "      weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
